{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import urllib\n",
    "import requests\n",
    "import joblib\n",
    "import re\n",
    "\n",
    "from maleo.wizard import Wizard\n",
    "from tqdm import tqdm\n",
    "from gensim.matutils import jaccard\n",
    "from strsimpy.levenshtein import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Political Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialPoliticalModel():    \n",
    "    def __init__(self, model, spwd):\n",
    "        self.model = joblib.load(model)\n",
    "        self.spwd = pd.read_csv(spwd, usecols = ['social political'])\n",
    "        \n",
    "    def tokenizer(self, text):\n",
    "        output = []\n",
    "        list_token = text.split('_')\n",
    "        list_token = list(filter(None, list_token))\n",
    "        list_token = [token[1:] if token.startswith('@') else token for token in list_token]\n",
    "\n",
    "        pattern1 = r\"([A-Z][a-z])\"\n",
    "        pattern2 = r\"([A-Z]{2,})\"\n",
    "        insert_space = r\" \\1\"\n",
    "\n",
    "        step1 = [re.sub(pattern1, insert_space, token) for token in list_token]\n",
    "        step2 = [re.sub(pattern2, insert_space, token).split() for token in step1]\n",
    "\n",
    "        for i in step2:\n",
    "            output += i\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def jaccard_sim_feature(self, token, spwd):    \n",
    "        jaccard_user = []\n",
    "        for user_token in token:\n",
    "            jaccard_val = []\n",
    "            for word in spwd['social political']:\n",
    "                jaccard_val.append(jaccard(user_token, word))\n",
    "            jaccard_user.append(min(jaccard_val))\n",
    "        return min(jaccard_user)\n",
    "    \n",
    "    def levenshtein_feature(self, token, spwd):\n",
    "        levenshtein = Levenshtein()\n",
    "        levenshtein_user = []\n",
    "        for user_token in token:\n",
    "            levenshtein_val = []\n",
    "            for word in spwd['social political']:\n",
    "                levenshtein_val.append(levenshtein.distance(user_token, word))\n",
    "            levenshtein_user.append(min(levenshtein_val))\n",
    "        return min(levenshtein_user)\n",
    "        \n",
    "    def features(self, text):\n",
    "        token = self.tokenizer(text)\n",
    "        char_count = len(text)\n",
    "        jaccard_sim = self.jaccard_sim_feature(token, self.spwd)\n",
    "        levenshtein_dist = self.levenshtein_feature(token, self.spwd)\n",
    "        \n",
    "        return jaccard_sim, levenshtein_dist, char_count\n",
    "    \n",
    "    def predict(self, text):\n",
    "        jaccard, levenshtein, char_count = self.features(text)\n",
    "        feat_dict = {'jaccard_sim': [jaccard], \n",
    "                     'levenshtein_dist': [levenshtein], \n",
    "                     'char_count': [char_count]}\n",
    "        feat = pd.DataFrame.from_dict(feat_dict)\n",
    "        output = self.model.predict(feat)[0]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuzzerFeatures():\n",
    "    def __init__(self, data_path, profile_data_path, sp_model):\n",
    "        self.data_path = data_path\n",
    "        self.profile_data_path = profile_data_path\n",
    "        self.sp_model = sp_model\n",
    "        self.wiz = Wizard()\n",
    "        self.feat = []\n",
    "    \n",
    "    \n",
    "    def read_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    \n",
    "    def write_json(self, path, data):\n",
    "        with open(path, 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "    \n",
    "    \n",
    "    def separate_tweets(self, user_data):\n",
    "        \"\"\"Data pada key \"tweets\" terdiri atas independent & dependent tweet.\n",
    "\n",
    "        Independent tweet = tweet yang dibuat sendiri (inspirasi sendiri)\n",
    "        Dependent tweet = tweet yang mengutip/quote tweet org lain (quoted tweet)\"\"\"\n",
    "\n",
    "        list_tweets, list_quoted_tweets = [], []\n",
    "\n",
    "        for twt in user_data['tweets']:\n",
    "            list_tweets.append(twt['full_text'])\n",
    "            try:\n",
    "                list_quoted_tweets.append(twt['quoted_status']['full_text'])\n",
    "            except:\n",
    "                pass\n",
    "        return list_tweets, list_quoted_tweets\n",
    "    \n",
    "    \n",
    "    def get_all_hashtag(self, list_tweets):   \n",
    "        all_hashtag = []\n",
    "\n",
    "        wiz = Wizard()\n",
    "        twt_hashtag = wiz.get_hashtag(pd.Series(list_tweets))['Hashtag']\n",
    "        n_twt_use_hashtag = len(twt_hashtag)\n",
    "\n",
    "        for i in twt_hashtag:\n",
    "            all_hashtag += i\n",
    "        return n_twt_use_hashtag, all_hashtag\n",
    "    \n",
    "    \n",
    "    def hashtag_related_feat(self, list_tweets):\n",
    "        n_twt_use_hashtag, all_hashtag = self.get_all_hashtag(list_tweets)\n",
    "\n",
    "        if n_twt_use_hashtag != 0:\n",
    "            ratio = (n_twt_use_hashtag/len(list_tweets))\n",
    "        else:\n",
    "            ratio = 0\n",
    "        return all_hashtag, n_twt_use_hashtag, ratio\n",
    "    \n",
    "    \n",
    "    def get_desc(self, filename, user_data, username_desc):\n",
    "        username = filename.split('/')[-1][:-5]\n",
    "        if not username.startswith('@'):\n",
    "            try:\n",
    "                desc = username_desc.get(username)[1]\n",
    "            except:\n",
    "                desc = ''\n",
    "        else:\n",
    "            try:\n",
    "                desc = user_data['description']\n",
    "            except:\n",
    "                desc = ''\n",
    "        return username, desc\n",
    "    \n",
    "    \n",
    "    def get_media_and_url(self, data):\n",
    "        media_type = None\n",
    "        url_link = None\n",
    "\n",
    "        if 'quoted_status' not in data:\n",
    "            try:\n",
    "                media_type = data['extended_entities']['media'][0]['type']\n",
    "            except:\n",
    "                pass\n",
    "            if media_type != 'photo' and data['entities']['urls'] != []:\n",
    "                url_link = data['entities']['urls'][0]['expanded_url']\n",
    "        return media_type, url_link\n",
    "        \n",
    "        \n",
    "    def extract_url_title(self, data):\n",
    "        media_type, url_link = self.get_media_and_url(data)\n",
    "\n",
    "        if url_link is None:\n",
    "            content_url = None\n",
    "        else:\n",
    "            content_url = url_link\n",
    "        return media_type, content_url\n",
    "    \n",
    "    \n",
    "    def summary_media_content(self, user_data):\n",
    "        media_content = [self.extract_url_title(twt) for twt in user_data]\n",
    "        \n",
    "        if media_content != []:        \n",
    "            media_type, content_url = zip(*media_content)\n",
    "            n_photo = media_type.count('photo')\n",
    "            n_video = media_type.count('video')\n",
    "            content_url = [item for item in content_url if item is not None]\n",
    "        else:\n",
    "            n_photo, n_video, content_url = None, None, None\n",
    "        return n_photo, n_video, content_url\n",
    "    \n",
    "    \n",
    "    def feature_extraction(self):\n",
    "        self.feat = []\n",
    "        for filename in tqdm(glob.glob(self.data_path)):\n",
    "            user_data = self.read_json(filename)\n",
    "            \n",
    "            # Checker\n",
    "            self.error_code = self.error_code_checker(user_data)\n",
    "            if self.error_code:\n",
    "                continue\n",
    "            \n",
    "            # Separate tweets\n",
    "            list_tweets, list_quoted_tweets = self.separate_tweets(user_data)\n",
    "            # Extract hashtag related features\n",
    "            if list_tweets:\n",
    "                all_hashtag, n_twt_use_hashtag, ratio = self.hashtag_related_feat(list_tweets)\n",
    "            else:\n",
    "                all_hashtag = []\n",
    "                n_twt_use_hashtag, ratio = 0, 0\n",
    "            \n",
    "            # Get username description\n",
    "            profile_id = self.read_json(profile_data_path)\n",
    "            username_desc = {user['screen_name']:(user['name'], user['description']) for user in profile_id}\n",
    "            username, desc = self.get_desc(filename, user_data, username_desc)\n",
    "            # Get summary of media content\n",
    "            n_photo, n_video, content_url = self.summary_media_content(user_data['tweets'])\n",
    "            \n",
    "            try:\n",
    "                name = username_desc.get(username)[0]\n",
    "            except:\n",
    "                if user_data['tweets']:\n",
    "                    name = user_data['tweets'][0]['user']['name']\n",
    "                else:\n",
    "                    name = user_data['retweets'][0]['user']['name']\n",
    "            \n",
    "            is_name_sp = self.sp_model.predict(name)\n",
    "            \n",
    "            # Output\n",
    "            out = {'username': username,\n",
    "                   'name': name,\n",
    "                   'is_name_social_political': int(is_name_sp),\n",
    "                   'desc': desc,\n",
    "                   'tweets': list_tweets,\n",
    "                   'n_tweet': len(list_tweets),\n",
    "                   'quoted_tweets': list_quoted_tweets,\n",
    "                   'hashtag': all_hashtag,\n",
    "                   'n_tweet_use_hashtag': n_twt_use_hashtag,\n",
    "                   'ratio_tweets_use_hashtag': ratio,\n",
    "                   'n_photo': n_photo,\n",
    "                   'n_video': n_video,\n",
    "                   'content_url': content_url}\n",
    "            self.feat.append(out)\n",
    "    \n",
    "    \n",
    "    def data_preprocessing(self, data):\n",
    "        if data:\n",
    "            data = pd.Series(data)\n",
    "            out = self.wiz.rm_link(data)\n",
    "            out = self.wiz.rm_non_ascii(out)\n",
    "            out = self.wiz.rm_punc(out)\n",
    "            out = self.wiz.slang_to_formal(out)\n",
    "            out = self.wiz.rm_stopword(out)\n",
    "            out = out.astype(str).str.strip()\n",
    "            out = self.wiz.rm_multiple_space(out)\n",
    "            out = out.apply(str.lower)\n",
    "            return out.tolist()\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "\n",
    "    def features(self, processed=False):\n",
    "        if processed:\n",
    "            self.feature_extraction()\n",
    "            for user_feat in tqdm(self.feat):\n",
    "                try:\n",
    "                    user_feat['desc'] = self.data_preprocessing(user_feat['desc'])[0]\n",
    "                    user_feat['tweets'] = self.data_preprocessing(user_feat['tweets'])\n",
    "                    user_feat['tweets'] = list(filter(None, user_feat['tweets']))\n",
    "                    user_feat['quoted_tweets'] = self.data_preprocessing(user_feat['quoted_tweets'])\n",
    "                    user_feat['quoted_tweets'] = list(filter(None, user_feat['quoted_tweets']))\n",
    "                except:\n",
    "                    pass\n",
    "            print('Get clean features')\n",
    "        else:\n",
    "            self.feature_extraction()\n",
    "            print('Get raw features')\n",
    "    \n",
    "    \n",
    "    def error_code_checker(self, user_data):\n",
    "        error_code = [\"401 : account_suspended_or_locked\", \"404 : account_not_found\"]\n",
    "        if user_data['error_code'] in error_code or user_data['status_count'] == 0:\n",
    "            keys = ['username', 'name', 'is_name_social_political', 'desc', \n",
    "                    'tweets', 'n_tweet', 'quoted_tweets', 'hashtag', 'n_tweet_use_hashtag',\n",
    "                    'ratio_tweets_use_hashtag', 'n_photo', 'n_video', 'content_url']\n",
    "            out = {key:None for key in keys}\n",
    "            self.feat.append(out)\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/data_7200/*.json'\n",
    "profile_data_path = '../data/profile_id.json'\n",
    "sp_model_path = '../model/social_political_clf.pkl'\n",
    "spwd_path = '../data/SPWD.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = SocialPoliticalModel(model=sp_model_path, spwd=spwd_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzzer = BuzzerFeatures(data_path, profile_data_path, sp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzzer.features(processed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buzzer.write_json('../data/dataset/buzzer_features.json', buzzer.feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
